{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### (deploy-server) Start MngoDB\n",
    "\n",
    "export WORKDIR='/root/PySpark/Step3_setup_spark_cluster/5_Spark/'\n",
    "cd $WORKDIR\n",
    "docker-compose -f mongo-docker-compose.yml up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Spark UI http://localhost:9090/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2860\r\n",
      "drwxr-xr-x  5 root root    4096 Mar 19 12:42  .\r\n",
      "drwxr-xr-x 14 root root    4096 Mar 19 11:40  ..\r\n",
      "drwxr-xr-x  2 root root    4096 Mar 19 12:26  .ipynb_checkpoints\r\n",
      "-rw-r--r--  1 root root  222666 Mar 17 13:32 'Pyspark Data Pipeline.ipynb'\r\n",
      "drwxr-xr-x  3 root root    4096 Mar 15 08:23  PysparkQuakes\r\n",
      "-rw-r--r--  1 root root    4005 Mar 15 08:23 'Start Services in Cluster.md'\r\n",
      "drwxr-xr-x  2 root root    4096 Mar 15 08:23  codes\r\n",
      "-rw-r--r--  1 root root     530 Mar 15 08:23  commands.txt\r\n",
      "-rw-r--r--  1 root root   62680 Mar 17 13:30  dashboard.html\r\n",
      "-rw-r--r--  1 root root 2397103 Mar 15 08:23  database.csv\r\n",
      "-rw-r--r--  1 root root    1975 Mar 19 12:36  extracting.py\r\n",
      "-rw-r--r--  1 root root     616 Mar 19 12:09  predict.py\r\n",
      "-rw-r--r--  1 root root    1605 Mar 19 12:08  preprocessing-4-ml.py\r\n",
      "-rw-r--r--  1 root root    2407 Mar 19 12:19  preprocessing.py\r\n",
      "-rw-r--r--  1 root root   72760 Mar 15 08:23  query.csv\r\n",
      "-rw-r--r--  1 root root   32991 Mar 19 12:14 'step1_DAG_Pyspark Data Pipeline.ipynb'\r\n",
      "-rw-r--r--  1 root root   26878 Mar 19 12:24 'step2_DAG_Pyspark Data Pipeline.ipynb'\r\n",
      "-rw-r--r--  1 root root   31764 Mar 19 12:42 'step3_DAG_Pyspark Data Pipeline.ipynb'\r\n",
      "-rw-r--r--  1 root root     971 Mar 19 12:09  train.py\r\n",
      "-rw-r--r--  1 root root    8929 Mar 19 12:14  visualize.py\r\n",
      "-rw-r--r--  1 root root     454 Mar 19 12:05  write-to-mongo.py\r\n"
     ]
    }
   ],
   "source": [
    "! ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../dags/elt_spark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../dags/elt_spark.py\n",
    "\n",
    "import airflow\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from pyspark.sql import SparkSession, functions\n",
    "from ./preprocessing import *\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'jih',\n",
    "    'start_date': datetime(2020, 11, 18),\n",
    "    'retries': 10,\n",
    "    'retry_delay': timedelta(hours=1)\n",
    "}\n",
    "with airflow.DAG('dag_etl_spark',\n",
    "                  default_args=default_args,\n",
    "                  schedule_interval='0 1 * * *') as dag:\n",
    "    task_elt_documento_pagar = PythonOperator(\n",
    "        task_id='elt_spark',\n",
    "        python_callable=processo_etl_spark\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../dags/spark-mongo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../dags/spark-mongo.py\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, functions\n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import pyspark\n",
    "    \n",
    "default_args = {\n",
    "    'start_date': datetime(2022, 1, 1)\n",
    "}\n",
    "\n",
    "with DAG(dag_id='pre_processing',\n",
    "         schedule_interval=\"@daily\",\n",
    "         default_args=default_args,\n",
    "         tags=['spark'],\n",
    "         catchup=False) as dag:\n",
    "    # proprocess\n",
    "    preprocess = PythonOperator(\n",
    "        task_id=\"spark_etl\",\n",
    "        python_callable=etl_spark\n",
    "    )\n",
    "    \n",
    "def etl_spark():\n",
    "\n",
    "\n",
    "    ### Configure spark session\n",
    "    spark = SparkSession\\\n",
    "       .builder\\\n",
    "       .master('spark://master:7077')\\\n",
    "       .appName('quake_etl')\\\n",
    "       .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1')\\\n",
    "       .getOrCreate()\n",
    "\n",
    "    #spark.sparkContext._conf.getAll()\n",
    "\n",
    "    # Load the dataset \n",
    "    df_load = spark.read.csv('hdfs://master:9000/database.csv', header=True)\n",
    "\n",
    "    # Drop fields we don't need from df_load\n",
    "    lst_dropped_columns = ['Depth Error', 'Time', 'Depth Seismic Stations','Magnitude Error','Magnitude Seismic Stations','Azimuthal Gap', 'Horizontal Distance','Horizontal Error',\n",
    "        'Root Mean Square','Source','Location Source','Magnitude Source','Status']\n",
    "    df_load = df_load.drop(*lst_dropped_columns)\n",
    "\n",
    "    # Create a year field and add it to the dataframe\n",
    "    df_load = df_load.withColumn('Year', year(to_timestamp('Date', 'dd/MM/yyyy')))\n",
    "\n",
    "    # Build the quakes frequency dataframe using the year field and counts for each year\n",
    "    df_quake_freq = df_load.groupBy('Year').count().withColumnRenamed('count', 'Counts')\n",
    "\n",
    "    # Cast some fields from string into numeric types\n",
    "    df_load = df_load.withColumn('Latitude', df_load['Latitude'].cast(DoubleType()))\\\n",
    "        .withColumn('Longitude', df_load['Longitude'].cast(DoubleType()))\\\n",
    "        .withColumn('Depth', df_load['Depth'].cast(DoubleType()))\\\n",
    "        .withColumn('Magnitude', df_load['Magnitude'].cast(DoubleType()))\n",
    "\n",
    "    # Create avg magnitude and max magnitude fields and add to df_quake_freq\n",
    "    df_max = df_load.groupBy('Year').max('Magnitude').withColumnRenamed('max(Magnitude)', 'Max_Magnitude')\n",
    "    df_avg = df_load.groupBy('Year').avg('Magnitude').withColumnRenamed('avg(Magnitude)', 'Avg_Magnitude')\n",
    "\n",
    "    # Join df_max, and df_avg to df_quake_freq\n",
    "    df_quake_freq = df_quake_freq.join(df_avg, ['Year']).join(df_max, ['Year'])\n",
    "\n",
    "    # Remove nulls\n",
    "    df_load.dropna()\n",
    "    df_quake_freq.dropna()\n",
    "\n",
    "    # Build the tables/collections in mongodb\n",
    "    # Write df_load to mongodb\n",
    "    df_load.write.format('mongo')\\\n",
    "        .mode('overwrite')\\\n",
    "        .option('spark.mongodb.output.uri', 'mongodb://root:go2team@mongo:27017/Quake.quakes?authSource=admin').save()\n",
    "\n",
    "    # Write df_quake_freq to mongodb\n",
    "    df_quake_freq.write.format('mongo')\\\n",
    "        .mode('overwrite')\\\n",
    "        .option('spark.mongodb.output.uri', 'mongodb://root:go2team@mongo:27017/Quake.quake_freq?authSource=admin').save()\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../dags/spark-mongo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../dags/spark-mongo.py\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "    'start_date': datetime(2022, 1, 1)\n",
    "}\n",
    "\n",
    "with DAG(dag_id='proprocessing',\n",
    "         schedule_interval=\"@daily\",\n",
    "         default_args=default_args,\n",
    "         tags=['spark'],\n",
    "         catchup=False) as dag:\n",
    "    # proprocess\n",
    "    preprocess = SparkSubmitOperator(\n",
    "        application=\"../preprocessing.py\",\n",
    "        task_id=\"preprocess\",\n",
    "        conn_id=\"spark_cluster\",\n",
    "    )\n",
    "    \n",
    "#     # extracting\n",
    "#     extract = SparkSubmitOperator(\n",
    "#         application=\"../extracting.py\",\n",
    "#         task_id=\"extract\",\n",
    "#         conn_id=\"spark_cluster\"\n",
    "#     )    \n",
    "#     # training\n",
    "#     train = SparkSubmitOperator(\n",
    "#         application=\"../training.py\",\n",
    "#         task_id=\"train\",\n",
    "#         conn_id=\"spark_cluster\"\n",
    "#     )\n",
    "    \n",
    "#     preprocess >> extract >> extract\n",
    "\n",
    "\n",
    "def process-etl-spark():\n",
    "    import pyspark\n",
    "    from pyspark.sql import SparkSession  \n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql.functions import *\n",
    "\n",
    "    ### Configure spark session\n",
    "    spark = SparkSession\\\n",
    "       .builder\\\n",
    "       .master('spark://master:7077')\\\n",
    "       .appName('quake_etl')\\\n",
    "       .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1')\\\n",
    "       .getOrCreate()\n",
    "\n",
    "    #spark.sparkContext._conf.getAll()\n",
    "\n",
    "    # Load the dataset \n",
    "    df_load = spark.read.csv('hdfs://master:9000/database.csv', header=True)\n",
    "\n",
    "    # Drop fields we don't need from df_load\n",
    "    lst_dropped_columns = ['Depth Error', 'Time', 'Depth Seismic Stations','Magnitude Error','Magnitude Seismic Stations','Azimuthal Gap', 'Horizontal Distance','Horizontal Error',\n",
    "        'Root Mean Square','Source','Location Source','Magnitude Source','Status']\n",
    "    df_load = df_load.drop(*lst_dropped_columns)\n",
    "\n",
    "    # Create a year field and add it to the dataframe\n",
    "    df_load = df_load.withColumn('Year', year(to_timestamp('Date', 'dd/MM/yyyy')))\n",
    "\n",
    "    # Build the quakes frequency dataframe using the year field and counts for each year\n",
    "    df_quake_freq = df_load.groupBy('Year').count().withColumnRenamed('count', 'Counts')\n",
    "\n",
    "    # Cast some fields from string into numeric types\n",
    "    df_load = df_load.withColumn('Latitude', df_load['Latitude'].cast(DoubleType()))\\\n",
    "        .withColumn('Longitude', df_load['Longitude'].cast(DoubleType()))\\\n",
    "        .withColumn('Depth', df_load['Depth'].cast(DoubleType()))\\\n",
    "        .withColumn('Magnitude', df_load['Magnitude'].cast(DoubleType()))\n",
    "\n",
    "    # Create avg magnitude and max magnitude fields and add to df_quake_freq\n",
    "    df_max = df_load.groupBy('Year').max('Magnitude').withColumnRenamed('max(Magnitude)', 'Max_Magnitude')\n",
    "    df_avg = df_load.groupBy('Year').avg('Magnitude').withColumnRenamed('avg(Magnitude)', 'Avg_Magnitude')\n",
    "\n",
    "    # Join df_max, and df_avg to df_quake_freq\n",
    "    df_quake_freq = df_quake_freq.join(df_avg, ['Year']).join(df_max, ['Year'])\n",
    "\n",
    "    # Remove nulls\n",
    "    df_load.dropna()\n",
    "    df_quake_freq.dropna()\n",
    "\n",
    "    # Build the tables/collections in mongodb\n",
    "    # Write df_load to mongodb\n",
    "    df_load.write.format('mongo')\\\n",
    "        .mode('overwrite')\\\n",
    "        .option('spark.mongodb.output.uri', 'mongodb://root:go2team@mongo:27017/Quake.quakes?authSource=admin').save()\n",
    "\n",
    "    # Write df_quake_freq to mongodb\n",
    "    df_quake_freq.write.format('mongo')\\\n",
    "        .mode('overwrite')\\\n",
    "        .option('spark.mongodb.output.uri', 'mongodb://root:go2team@mongo:27017/Quake.quake_freq?authSource=admin').save()\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/spark-3.0.3-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-02a6fcc2-6744-4cb2-ae2b-f245e28018cb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 237ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-02a6fcc2-6744-4cb2-ae2b-f245e28018cb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/9ms)\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/usr/local/spark/python/pyspark/context.py:227: DeprecationWarning: Support for Python 2 and Python 3 prior to version 3.6 is deprecated as of Spark 3.0. See also the plan for dropping Python 2 support at https://spark.apache.org/news/plan-for-dropping-python-2-support.html.\n",
      "  DeprecationWarning)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "! python preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "## Load Packages\n",
    "\n",
    "def process_etl_spark():\n",
    "    import pyspark\n",
    "    from pyspark.sql import SparkSession  \n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql.functions import *\n",
    "\n",
    "    ### Configure spark session\n",
    "    spark = SparkSession\\\n",
    "       .builder\\\n",
    "       .master('spark://master:7077')\\\n",
    "       .appName('quake_etl')\\\n",
    "       .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1')\\\n",
    "       .getOrCreate()\n",
    "\n",
    "    #spark.sparkContext._conf.getAll()\n",
    "\n",
    "    # Load the dataset \n",
    "    df_load = spark.read.csv('hdfs://master:9000/database.csv', header=True)\n",
    "\n",
    "    # Drop fields we don't need from df_load\n",
    "    lst_dropped_columns = ['Depth Error', 'Time', 'Depth Seismic Stations','Magnitude Error','Magnitude Seismic Stations','Azimuthal Gap', 'Horizontal Distance','Horizontal Error',\n",
    "        'Root Mean Square','Source','Location Source','Magnitude Source','Status']\n",
    "    df_load = df_load.drop(*lst_dropped_columns)\n",
    "\n",
    "    # Create a year field and add it to the dataframe\n",
    "    df_load = df_load.withColumn('Year', year(to_timestamp('Date', 'dd/MM/yyyy')))\n",
    "\n",
    "    # Build the quakes frequency dataframe using the year field and counts for each year\n",
    "    df_quake_freq = df_load.groupBy('Year').count().withColumnRenamed('count', 'Counts')\n",
    "\n",
    "    # Cast some fields from string into numeric types\n",
    "    df_load = df_load.withColumn('Latitude', df_load['Latitude'].cast(DoubleType()))\\\n",
    "        .withColumn('Longitude', df_load['Longitude'].cast(DoubleType()))\\\n",
    "        .withColumn('Depth', df_load['Depth'].cast(DoubleType()))\\\n",
    "        .withColumn('Magnitude', df_load['Magnitude'].cast(DoubleType()))\n",
    "\n",
    "    # Create avg magnitude and max magnitude fields and add to df_quake_freq\n",
    "    df_max = df_load.groupBy('Year').max('Magnitude').withColumnRenamed('max(Magnitude)', 'Max_Magnitude')\n",
    "    df_avg = df_load.groupBy('Year').avg('Magnitude').withColumnRenamed('avg(Magnitude)', 'Avg_Magnitude')\n",
    "\n",
    "    # Join df_max, and df_avg to df_quake_freq\n",
    "    df_quake_freq = df_quake_freq.join(df_avg, ['Year']).join(df_max, ['Year'])\n",
    "\n",
    "    # Remove nulls\n",
    "    df_load.dropna()\n",
    "    df_quake_freq.dropna()\n",
    "\n",
    "    # Build the tables/collections in mongodb\n",
    "    # Write df_load to mongodb\n",
    "    df_load.write.format('mongo')\\\n",
    "        .mode('overwrite')\\\n",
    "        .option('spark.mongodb.output.uri', 'mongodb://root:go2team@mongo:27017/Quake.quakes?authSource=admin').save()\n",
    "\n",
    "    # Write df_quake_freq to mongodb\n",
    "    df_quake_freq.write.format('mongo')\\\n",
    "        .mode('overwrite')\\\n",
    "        .option('spark.mongodb.output.uri', 'mongodb://root:go2team@mongo:27017/Quake.quake_freq?authSource=admin').save()\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/spark-3.0.3-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-78e041f5-fd80-48ba-8e43-4f83367a9e8f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 254ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-78e041f5-fd80-48ba-8e43-4f83367a9e8f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/9ms)\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/usr/local/spark/python/pyspark/context.py:227: DeprecationWarning: Support for Python 2 and Python 3 prior to version 3.6 is deprecated as of Spark 3.0. See also the plan for dropping Python 2 support at https://spark.apache.org/news/plan-for-dropping-python-2-support.html.\n",
      "  DeprecationWarning)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "! python extracting.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting extracting.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile extracting.py\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "### Configure spark session\n",
    "spark = SparkSession\\\n",
    "   .builder\\\n",
    "   .master('spark://master:7077')\\\n",
    "   .appName('quake_etl')\\\n",
    "   .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1')\\\n",
    "   .getOrCreate()\n",
    "\n",
    "# Load the test data file into a dataframe\n",
    "df_test = spark.read.csv('hdfs://master:9000/query.csv', header=True)\n",
    "\n",
    "# Load the training data from mongo into a dataframe\n",
    "df_train = spark.read.format('mongo')\\\n",
    "    .option('spark.mongodb.input.uri', 'mongodb://root:go2team@mongo:27017/Quake.quakes?authSource=admin').load()\n",
    "\n",
    "# Select fields we will use and discard fields we don't need\n",
    "df_test_clean = df_test['time', 'latitude', 'longitude', 'mag', 'depth']\n",
    "\n",
    "# Rename fields\n",
    "df_test_clean = df_test_clean.withColumnRenamed('time', 'Date')\\\n",
    "    .withColumnRenamed('latitude', 'Latitude')\\\n",
    "    .withColumnRenamed('longitude', 'Longitude')\\\n",
    "    .withColumnRenamed('mag', 'Magnitude')\\\n",
    "    .withColumnRenamed('depth', 'Depth')\n",
    "\n",
    "# Cast some string fields into numeric fields\n",
    "df_test_clean = df_test_clean.withColumn('Latitude', df_test_clean['Latitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Longitude', df_test_clean['Longitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Depth', df_test_clean['Depth'].cast(DoubleType()))\\\n",
    "    .withColumn('Magnitude', df_test_clean['Magnitude'].cast(DoubleType()))\n",
    "\n",
    "# Create training and testing dataframes\n",
    "df_testing = df_test_clean['Latitude', 'Longitude', 'Magnitude', 'Depth']\n",
    "df_training = df_train['Latitude', 'Longitude', 'Magnitude', 'Depth']\n",
    "\n",
    "# Drop records with null values from our dataframes\n",
    "df_testing = df_testing.dropna()\n",
    "df_training = df_training.dropna()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Select features to parse into our model and then create the feature vector\n",
    "assembler = VectorAssembler(inputCols=['Latitude', 'Longitude', 'Depth'], outputCol='features')\n",
    "\n",
    "# Create the Model\n",
    "model_reg = RandomForestRegressor(featuresCol='features', labelCol='Magnitude')\n",
    "\n",
    "# Chain the assembler with the model in a pipeline\n",
    "pipeline = Pipeline(stages=[assembler, model_reg])\n",
    "\n",
    "# Train the Model\n",
    "model = pipeline.fit(df_training)\n",
    "\n",
    "# Make the prediction\n",
    "pred_results = model.transform(df_testing)\n",
    "\n",
    "# Evaluate the model\n",
    "# rmse should be less than 0.5 for the model to be useful\n",
    "evaluator = RegressionEvaluator(labelCol='Magnitude', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(pred_results)\n",
    "print('Root Mean Squared Error (RMSE) on test data = %g' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the prediction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict.py\n",
    "\n",
    "# Create the prediction dataset\n",
    "df_pred_results = pred_results['Latitude', 'Longitude', 'prediction']\n",
    "\n",
    "# Rename the prediction field\n",
    "df_pred_results = df_pred_results.withColumnRenamed('prediction', 'Pred_Magnitude')\n",
    "\n",
    "# Add more columns to our prediction dataset\n",
    "df_pred_results = df_pred_results.withColumn('Year', lit(2017))\\\n",
    "    .withColumn('RMSE', lit(rmse))\n",
    "\n",
    "# Load the prediction dataset into mongodb\n",
    "# Write df_pred_results\n",
    "df_pred_results.write.format('mongo')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('spark.mongodb.output.uri', 'mongodb://root:go2team@mongo:27017/Quake.pred_results?authSource=admin').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bokeh\n",
      "  Downloading bokeh-2.3.3.tar.gz (10.7 MB)\n",
      "     |################################| 10.7 MB 7.8 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh) (6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.8.2)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.6/dist-packages (from bokeh) (3.0.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from bokeh) (1.19.5)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.6/dist-packages (from bokeh) (8.4.0)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh) (21.3)\n",
      "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (6.1)\n",
      "Requirement already satisfied: typing_extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from bokeh) (4.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.9->bokeh) (2.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=16.8->bokeh) (3.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->bokeh) (1.16.0)\n",
      "Building wheels for collected packages: bokeh\n",
      "  Building wheel for bokeh (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bokeh: filename=bokeh-2.3.3-py3-none-any.whl size=11342785 sha256=e8a9257d8bb2c3c397683fed1dffe6e356d22a20253a9cf249c59fb87dfd893d\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/59/97/257265b741bab184e0cc8f5676309cb1fe6fbda22011bbb3ff\n",
      "Successfully built bokeh\n",
      "Installing collected packages: bokeh\n",
      "Successfully installed bokeh-2.3.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install pandas pymongo bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing visualize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile visualize.py\n",
    "\n",
    "import pandas as pd\n",
    "from bokeh.io import output_notebook, output_file\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.models.tools import HoverTool\n",
    "import math\n",
    "from math import pi\n",
    "from bokeh.palettes import Category20c\n",
    "from bokeh.transform import cumsum\n",
    "from bokeh.tile_providers import CARTODBPOSITRON\n",
    "from bokeh.tile_providers import get_provider, Vendors\n",
    "from bokeh.themes import built_in_themes\n",
    "from bokeh.io import curdoc\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Create a custom read function to read data from mongodb into a dataframe\n",
    "#def read_mongo(host='127.0.0.1', port=27017, username=None, password=None, db='Quake', collection='pred_results'):\n",
    "def read_mongo(host='mongo', port=27017, username='root', password='go2team', db='Quake', collection='pred_results'):\n",
    "    \n",
    "    mongo_uri = 'mongodb://{}:{}@{}:{}/{}.{}?authSource=admin'.format(username, password, host, port, db, collection)\n",
    "    \n",
    "    # Connect to mongodb\n",
    "    conn = MongoClient(mongo_uri)\n",
    "    db = conn[db]\n",
    "    \n",
    "    # Select all records from the collection\n",
    "    cursor = db[collection].find()\n",
    "    \n",
    "    # Create the dataframe\n",
    "    df = pd.DataFrame(list(cursor))\n",
    "    \n",
    "    # Delete the _id field\n",
    "    del df['_id']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Load the datasets from mongodb\n",
    "df_quakes = read_mongo(collection='quakes')\n",
    "df_quake_freq = read_mongo(collection='quake_freq')\n",
    "df_quake_pred = read_mongo(collection='pred_results')\n",
    "df_quakes_2016 = df_quakes[df_quakes['Year'] == 2016]\n",
    "\n",
    "\n",
    "# Create custom style function to style our plots\n",
    "def style(p):\n",
    "    # Title\n",
    "    p.title.align='center'\n",
    "    p.title.text_font_size='20pt'\n",
    "    p.title.text_font='serif'\n",
    "    \n",
    "    # Axis titles\n",
    "    p.xaxis.axis_label_text_font_size='14pt'\n",
    "    p.xaxis.axis_label_text_font_style='bold'\n",
    "    p.yaxis.axis_label_text_font_size='14pt'\n",
    "    p.yaxis.axis_label_text_font_style='bold'\n",
    "    \n",
    "    # Tick labels\n",
    "    p.xaxis.major_label_text_font_size='12pt'\n",
    "    p.yaxis.major_label_text_font_size='12pt'\n",
    "    \n",
    "    # Plot the legend in the top left corner\n",
    "    p.legend.location='top_left'\n",
    "    \n",
    "    return p\n",
    "    \n",
    "\n",
    "# Create the Geo Map plot\n",
    "def plotMap():\n",
    "    lat = df_quakes_2016['Latitude'].values.tolist()\n",
    "    lon = df_quakes_2016['Longitude'].values.tolist()\n",
    "    \n",
    "    pred_lat = df_quake_pred['Latitude'].values.tolist()\n",
    "    pred_lon = df_quake_pred['Longitude'].values.tolist()\n",
    "    \n",
    "    lst_lat = []\n",
    "    lst_lon = []\n",
    "    lst_pred_lat = []\n",
    "    lst_pred_lon = []\n",
    "    \n",
    "    i=0\n",
    "    j=0\n",
    "    \n",
    "    # Convert Lat and Long values into merc_projection format\n",
    "    for i in range(len(lon)):\n",
    "        r_major = 6378137.000\n",
    "        x = r_major * math.radians(lon[i])\n",
    "        scale = x/lon[i]\n",
    "        y = 180.0/math.pi * math.log(math.tan(math.pi/4.0 +\n",
    "            lat[i] * (math.pi/180.0)/2.0)) * scale\n",
    "        \n",
    "        lst_lon.append(x)\n",
    "        lst_lat.append(y)\n",
    "        i += 1\n",
    "        \n",
    "    # Convert predicted lat and long values into merc_projection format\n",
    "    for j in range(len(pred_lon)):\n",
    "        r_major = 6378137.000\n",
    "        x = r_major * math.radians(pred_lon[j])\n",
    "        scale = x/pred_lon[j]\n",
    "        y = 180.0/math.pi * math.log(math.tan(math.pi/4.0 +\n",
    "            pred_lat[j] * (math.pi/180.0)/2.0)) * scale\n",
    "        \n",
    "        lst_pred_lon.append(x)\n",
    "        lst_pred_lat.append(y)\n",
    "        j += 1\n",
    "    \n",
    "    \n",
    "    df_quakes_2016['coords_x'] = lst_lat\n",
    "    df_quakes_2016['coords_y'] = lst_lon\n",
    "    df_quake_pred['coords_x'] = lst_pred_lat\n",
    "    df_quake_pred['coords_y'] = lst_pred_lon\n",
    "    \n",
    "    # Scale the circles\n",
    "    df_quakes_2016['Mag_Size'] = df_quakes_2016['Magnitude'] * 4\n",
    "    df_quake_pred['Mag_Size'] = df_quake_pred['Pred_Magnitude'] * 4\n",
    "    \n",
    "    # create datasources for our ColumnDataSource object\n",
    "    lats = df_quakes_2016['coords_x'].tolist()\n",
    "    longs = df_quakes_2016['coords_y'].tolist()\n",
    "    mags = df_quakes_2016['Magnitude'].tolist()\n",
    "    years = df_quakes_2016['Year'].tolist()\n",
    "    mag_size = df_quakes_2016['Mag_Size'].tolist()\n",
    "    \n",
    "    pred_lats = df_quake_pred['coords_x'].tolist()\n",
    "    pred_longs = df_quake_pred['coords_y'].tolist()\n",
    "    pred_mags = df_quake_pred['Pred_Magnitude'].tolist()\n",
    "    pred_year = df_quake_pred['Year'].tolist()\n",
    "    pred_mag_size = df_quake_pred['Mag_Size'].tolist()\n",
    "    \n",
    "    # Create column datasource\n",
    "    cds = ColumnDataSource(\n",
    "        data=dict(\n",
    "            lat=lats,\n",
    "            lon=longs,\n",
    "            mag=mags,\n",
    "            year=years,\n",
    "            mag_s=mag_size\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    pred_cds = ColumnDataSource(\n",
    "        data=dict(\n",
    "            pred_lat=pred_lats,\n",
    "            pred_long=pred_longs,\n",
    "            pred_mag=pred_mags,\n",
    "            year=pred_year,\n",
    "            pred_mag_s=pred_mag_size\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Tooltips\n",
    "    TOOLTIPS = [\n",
    "        (\"Year\", \" @year\"),\n",
    "        (\"Magnitude\", \" @mag\"),\n",
    "        (\"Predicted Magnitude\", \" @pred_mag\")\n",
    "    ]\n",
    "    \n",
    "    # Create figure\n",
    "    p = figure(title='Earthquake Map',\n",
    "              plot_width=2300, plot_height=450,\n",
    "              x_range=(-2000000, 6000000),\n",
    "              y_range=(-1000000, 7000000),\n",
    "              tooltips=TOOLTIPS)\n",
    "    \n",
    "    p.circle(x='lon', y='lat', size='mag_s', fill_color='#cc0000', fill_alpha=0.7,\n",
    "            source=cds, legend='Quakes 2016')\n",
    "    \n",
    "    # Add circles for our predicted earthquakes\n",
    "    p.circle(x='pred_long', y='pred_lat', size='pred_mag_s', fill_color='#ccff33', fill_alpha=7.0,\n",
    "            source=pred_cds, legend='Predicted Quakes 2017')\n",
    "    \n",
    "    #p.add_tile(CARTODBPOSITRON)\n",
    "    p.add_tile(get_provider('CARTODBPOSITRON'))\n",
    "    \n",
    "    # Style the map plot\n",
    "    # Title\n",
    "    p.title.align='center'\n",
    "    p.title.text_font_size='20pt'\n",
    "    p.title.text_font='serif'\n",
    "    \n",
    "    # Legend\n",
    "    p.legend.location='bottom_right'\n",
    "    p.legend.background_fill_color='black'\n",
    "    p.legend.background_fill_alpha=0.8\n",
    "    p.legend.click_policy='hide'\n",
    "    p.legend.label_text_color='white'\n",
    "    p.xaxis.visible=False\n",
    "    p.yaxis.visible=False\n",
    "    p.axis.axis_label=None\n",
    "    p.axis.visible=False\n",
    "    p.grid.grid_line_color=None\n",
    "    \n",
    "       \n",
    "    # show(p)\n",
    "\n",
    "    return p\n",
    "    \n",
    "#plotMap()  \n",
    "\n",
    "# Create the Bar Chart\n",
    "def plotBar():\n",
    "    # Load the datasource \n",
    "    cds = ColumnDataSource(data=dict(\n",
    "        yrs = df_quake_freq['Year'].values.tolist(),\n",
    "        numQuakes = df_quake_freq['Counts'].values.tolist()\n",
    "    ))\n",
    "    \n",
    "    # Tooltip\n",
    "    TOOLTIPS = [\n",
    "        ('Year', ' @yrs'),\n",
    "        ('Number of earthquakes', ' @numQuakes')\n",
    "    ]\n",
    "    \n",
    "    # Create a figure\n",
    "    barChart = figure(title='Frequency of Earthquakes by Year',\n",
    "                     plot_height=400,\n",
    "                     plot_width=1150,\n",
    "                     x_axis_label='Years',\n",
    "                     y_axis_label='Number of Occurances',\n",
    "                     x_minor_ticks=2,\n",
    "                     y_range=(0, df_quake_freq['Counts'].max() + 100),\n",
    "                     toolbar_location=None,\n",
    "                     tooltips=TOOLTIPS)\n",
    "    \n",
    "    # Create a vertical bar \n",
    "    barChart.vbar(x='yrs', bottom=0, top='numQuakes',\n",
    "                 color='#cc0000', width=0.75,\n",
    "                 legend='Year', source=cds)\n",
    "    \n",
    "    # Style the bar chart\n",
    "    barChart = style(barChart)\n",
    "    \n",
    "    # show(barChart)\n",
    "    \n",
    "    return barChart\n",
    "    \n",
    "    \n",
    "# plotBar()\n",
    " \n",
    "\n",
    "\n",
    "df_quake_freq.sort_values(by=['Year'], axis=0, ascending=True, inplace=True)\n",
    "\n",
    "df_quake_freq.head()\n",
    "\n",
    "df_quake_freq.reset_index(drop=True)\n",
    "\n",
    "# Create a magnitude plot\n",
    "def plotMagnitude():\n",
    "    # Load the datasource\n",
    "    cds = ColumnDataSource(data=dict(\n",
    "        yrs = df_quake_freq['Year'].values.tolist(),\n",
    "        avg_mag = df_quake_freq['Avg_Magnitude'].round(1).values.tolist(),\n",
    "        max_mag = df_quake_freq['Max_Magnitude'].values.tolist()\n",
    "    ))\n",
    "    \n",
    "    # Tooltip\n",
    "    TOOLTIPS = [\n",
    "        ('Year', ' @yrs'),\n",
    "        ('Average Magnitude', ' @avg_mag'),\n",
    "        ('Maximum Magnitude', ' @max_mag')\n",
    "    ]\n",
    "    \n",
    "    # Create the figure\n",
    "    mp = figure(title='Maximum and Average Magnitude by Year',\n",
    "               plot_width=1150, plot_height=400,\n",
    "               x_axis_label='Years',\n",
    "               y_axis_label='Magnitude',\n",
    "               x_minor_ticks=2,\n",
    "               y_range=(5, df_quake_freq['Max_Magnitude'].max() + 1),\n",
    "               toolbar_location=None,\n",
    "               tooltips=TOOLTIPS)\n",
    "    \n",
    "    # Max Magnitude\n",
    "    mp.line(x='yrs', y='max_mag', color='#cc0000', line_width=2, legend='Max Magnitude', source=cds)\n",
    "    mp.circle(x='yrs', y='max_mag', color='#cc0000', size=8, fill_color='#cc0000', source=cds)\n",
    "    \n",
    "    # Average Magnitude \n",
    "    mp.line(x='yrs', y='avg_mag', color='yellow', line_width=2, legend='Avg Magnitude', source=cds)\n",
    "    mp.circle(x='yrs', y='avg_mag', color='yellow', size=8, fill_color='yellow', source=cds)\n",
    "    \n",
    "    mp = style(mp)\n",
    "    \n",
    "    show(mp)\n",
    "    \n",
    "    return mp\n",
    "\n",
    "plotMagnitude()    \n",
    "\n",
    "# Display the visuals directly in the browser\n",
    "output_file('dashboard.html')\n",
    "# Change to a dark theme\n",
    "curdoc().theme = 'dark_minimal'\n",
    "\n",
    "import bokeh\n",
    "print(bokeh.__version__)\n",
    "\n",
    "# Build the grid plot\n",
    "from bokeh.layouts import gridplot, grid\n",
    "\n",
    "# Make the grid\n",
    "grid = grid( [ plotMap(), [plotBar(), plotMagnitude()]])\n",
    "\n",
    "# Show the grid\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
