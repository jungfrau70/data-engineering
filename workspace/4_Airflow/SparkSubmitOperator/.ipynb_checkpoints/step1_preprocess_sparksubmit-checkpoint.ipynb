{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "705cbc11-9a7b-4677-b39b-a92929f5c785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: `/database.csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! docker exec master /opt/hadoop/bin/hdfs dfs -ls /database.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b319a4ee-dbe6-422b-b3f3-3db49262344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pipeline/bin:/opt/conda/condabin:/usr/lib/jvm/java-8-openjdk-amd64/bin:/opt/spark/bin:/opt/spark/sbin:/root/.vscode-server/bin/c722ca6c7eed3d7987c0d5c3df5c45f6b15e77d1/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\n"
     ]
    }
   ],
   "source": [
    "! docker exec airflow-webserver echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2637ff41-7f00-41c2-9352-d02363c4bfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 root supergroup    2397103 2022-03-20 14:36 /database.csv\n"
     ]
    }
   ],
   "source": [
    "! docker exec master /opt/hadoop/bin/hdfs dfs -ls /database.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccacc05d-f366-479d-bbcb-6889b479c552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\n",
      "drwxr-xr-x   - root supergroup          0 2022-03-26 04:44 /SparkSubmitOperator\n",
      "drwxr-xr-x   - root supergroup          0 2022-03-26 04:42 /apps\n",
      "-rw-r--r--   3 root supergroup    2397103 2022-03-26 06:20 /database.csv\n",
      "drwxr-xr-x   - root supergroup          0 2022-03-26 04:42 /spark-jars\n",
      "drwxr-xr-x   - root supergroup          0 2022-03-26 06:12 /spark-logs\n",
      "-rw-r--r--   3 root supergroup       2445 2022-03-26 06:19 /step1_preprocess.py\n"
     ]
    }
   ],
   "source": [
    "! docker cp ../../4_MongoDB/database.csv master:/root/\n",
    "! docker exec master /opt/hadoop/bin/hdfs dfs -put /root/database.csv /\n",
    "! docker exec master /opt/hadoop/bin/hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44b2d0ea-375e-4092-920a-5eefba6b4cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting step1_preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile step1_preprocess.py\n",
    "# %load 4_MongoDB/step1_preprocess.py\n",
    "## Load Packages\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.conf import SparkConf\n",
    "#from pyspark import SparkConf, SparkContext \n",
    "            \n",
    "### Configure spark session\n",
    "spark = SparkSession\\\n",
    "   .builder\\\n",
    "   .master('spark://master:7077')\\\n",
    "   .appName('quake_etl')\\\n",
    "   .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1')\\\n",
    "   .config('spark.cores.max', '3')\\\n",
    "   .config('spark.executor.memory', '2g')\\\n",
    "   .config('spark.driver.host', 'airflow-webserver')\\\n",
    "   .getOrCreate()\n",
    "\n",
    "#spark.sparkContext._conf.getAll()\n",
    "\n",
    "# Load the dataset \n",
    "df_load = spark.read.csv('hdfs://master:9000/database.csv', header=True)\n",
    "\n",
    "# Drop fields we don't need from df_load\n",
    "lst_dropped_columns = ['Depth Error', 'Time', 'Depth Seismic Stations','Magnitude Error','Magnitude Seismic Stations','Azimuthal Gap', 'Horizontal Distance','Horizontal Error',\n",
    "    'Root Mean Square','Source','Location Source','Magnitude Source','Status']\n",
    "df_load = df_load.drop(*lst_dropped_columns)\n",
    "\n",
    "# Create a year field and add it to the dataframe\n",
    "df_load = df_load.withColumn('Year', year(to_timestamp('Date', 'dd/MM/yyyy')))\n",
    "\n",
    "# Build the quakes frequency dataframe using the year field and counts for each year\n",
    "df_quake_freq = df_load.groupBy('Year').count().withColumnRenamed('count', 'Counts')\n",
    "\n",
    "# Cast some fields from string into numeric types\n",
    "df_load = df_load.withColumn('Latitude', df_load['Latitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Longitude', df_load['Longitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Depth', df_load['Depth'].cast(DoubleType()))\\\n",
    "    .withColumn('Magnitude', df_load['Magnitude'].cast(DoubleType()))\n",
    "\n",
    "# Create avg magnitude and max magnitude fields and add to df_quake_freq\n",
    "df_max = df_load.groupBy('Year').max('Magnitude').withColumnRenamed('max(Magnitude)', 'Max_Magnitude')\n",
    "df_avg = df_load.groupBy('Year').avg('Magnitude').withColumnRenamed('avg(Magnitude)', 'Avg_Magnitude')\n",
    "\n",
    "# Join df_max, and df_avg to df_quake_freq\n",
    "df_quake_freq = df_quake_freq.join(df_avg, ['Year']).join(df_max, ['Year'])\n",
    "\n",
    "# Remove nulls\n",
    "df_load.dropna()\n",
    "df_quake_freq.dropna()\n",
    "\n",
    "# Build the tables/collections in mongodb\n",
    "# Write df_load to mongodb\n",
    "df_load.write.format('mongo')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('spark.mongodb.output.uri', 'mongodb://root:go2team@mongo:27017/Quake.quakes?authSource=admin').save()\n",
    "\n",
    "# Write df_quake_freq to mongodb\n",
    "df_quake_freq.write.format('mongo')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('spark.mongodb.output.uri', 'mongodb://root:go2team@mongo:27017/Quake.quake_freq?authSource=admin').save()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30d1819c-9ab8-49b4-a91f-eceaaa1e4ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker exec master rm /root/step1_preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac7a23b7-c7c3-432e-8951-294987a1305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker cp step1_preprocess.py master:/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "189c9761-1344-464f-926a-ae6bdf627c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /step1_preprocess.py\n",
      "Found 7 items\n",
      "drwxr-xr-x   - root supergroup          0 2022-03-26 09:37 /SparkSubmitOperator\n",
      "drwxr-xr-x   - root supergroup          0 2022-03-18 01:48 /apps\n",
      "-rw-r--r--   3 root supergroup    2397103 2022-03-26 09:23 /database.csv\n",
      "drwxr-xr-x   - root supergroup          0 2022-03-18 01:49 /spark-jars\n",
      "drwxr-xr-x   - root supergroup          0 2022-03-26 09:33 /spark-logs\n",
      "-rw-r--r--   3 root supergroup       2815 2022-03-26 12:07 /step1_preprocess.py\n",
      "drwx-wx-wx   - root supergroup          0 2022-03-18 01:41 /tmp\n"
     ]
    }
   ],
   "source": [
    "! docker exec master /opt/hadoop/bin/hdfs dfs -rm /step1_preprocess.py \n",
    "! docker exec master /opt/hadoop/bin/hdfs dfs -put /root/step1_preprocess.py /\n",
    "! docker exec master /opt/hadoop/bin/hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "769390d5-c97f-422a-9692-4db311246dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# %load 4_MongoDB/step1_preprocess.py\n",
      "## Load Packages\n",
      "\n",
      "import pyspark\n",
      "from pyspark.sql import SparkSession  \n",
      "from pyspark.sql.types import *\n",
      "from pyspark.sql.functions import *\n",
      "from pyspark.conf import SparkConf\n",
      "#from pyspark import SparkConf, SparkContext \n",
      "            \n",
      "### Configure spark session\n",
      "spark = SparkSession\\\n",
      "   .builder\\\n",
      "   .master('spark://master:7077')\\\n",
      "   .appName('quake_etl')\\\n",
      "   .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1')\\\n",
      "   .config('spark.blockManager.port', '10025')\\\n",
      "   .config('park.driver.blockManager.port', '10026')\\\n",
      "   .config('spark.driver.port', '10027')\\\n",
      "   .config('spark.cores.max', '3')\\\n",
      "   .config('spark.executor.memory', '2g')\\\n",
      "   .config('spark.driver.host', 'airflow-webserver')\\\n",
      "   .getOrCreate()\n",
      "\n",
      "#spark.sparkContext._conf.getAll()\n",
      "\n",
      "# Load the dataset \n",
      "df_load = spark.read.csv('hdfs://master:9000/database.csv', header=True)\n",
      "\n",
      "# Drop fields we don't need from df_load\n",
      "lst_dropped_columns = ['Depth Error', 'Time', 'Depth Seismic Stations','Magnitude Error','Magnitude Seismic Stations','Azimuthal Gap', 'Horizontal Distance','Horizontal Error',\n",
      "    'Root Mean Square','Source','Location Source','Magnitude Source','Status']\n",
      "df_load = df_load.drop(*lst_dropped_columns)\n",
      "\n",
      "# Create a year field and add it to the dataframe\n",
      "df_load = df_load.withColumn('Year', year(to_timestamp('Date', 'dd/MM/yyyy')))\n",
      "\n",
      "# Build the quakes frequency dataframe using the year field and counts for each year\n",
      "df_quake_freq = df_load.groupBy('Year').count().withColumnRenamed('count', 'Counts')\n",
      "\n",
      "# Cast some fields from string into numeric types\n",
      "df_load = df_load.withColumn('Latitude', df_load['Latitude'].cast(DoubleType()))\\\n",
      "    .withColumn('Longitude', df_load['Longitude'].cast(DoubleType()))\\\n",
      "    .withColumn('Depth', df_load['Depth'].cast(DoubleType()))\\\n",
      "    .withColumn('Magnitude', df_load['Magnitude'].cast(DoubleType()))\n",
      "\n",
      "# Create avg magnitude and max magnitude fields and add to df_quake_freq\n",
      "df_max = df_load.groupBy('Year').max('Magnitude').withColumnRenamed('max(Magnitude)', 'Max_Magnitude')\n",
      "df_avg = df_load.groupBy('Year').avg('Magnitude').withColumnRenamed('avg(Magnitude)', 'Avg_Magnitude')\n",
      "\n",
      "# Join df_max, and df_avg to df_quake_freq\n",
      "df_quake_freq = df_quake_freq.join(df_avg, ['Year']).join(df_max, ['Year'])\n",
      "\n",
      "# Remove nulls\n",
      "df_load.dropna()\n",
      "df_quake_freq.dropna()\n",
      "\n",
      "# Build the tables/collections in mongodb\n",
      "# Write df_load to mongodb\n",
      "df_load.write.format('mongo')\\\n",
      "    .mode('overwrite')\\\n",
      "    .option('spark.mongodb.output.uri', 'mongodb://root:go2team@mongo:27017/Quake.quakes?authSource=admin').save()\n",
      "\n",
      "# Write df_quake_freq to mongodb\n",
      "df_quake_freq.write.format('mongo')\\\n",
      "    .mode('overwrite')\\\n",
      "    .option('spark.mongodb.output.uri', 'mongodb://root:go2team@mongo:27017/Quake.quake_freq?authSource=admin').save()\n",
      "\n",
      "spark.stop()\n"
     ]
    }
   ],
   "source": [
    "! docker exec master /opt/hadoop/bin/hdfs dfs -cat /step1_preprocess.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b57b19-ac74-40b3-b7ae-2d40922259a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dags/dag_basicsparksubmit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dags/dag_basicsparksubmit.py\n",
    "import airflow\n",
    "from datetime import timedelta\n",
    "from airflow import DAG\n",
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator \n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',    \n",
    "    #'start_date': airflow.utils.dates.days_ago(2),\n",
    "    # 'end_date': datetime(),\n",
    "    # 'depends_on_past': False,\n",
    "    # 'email': ['airflow@example.com'],\n",
    "    # 'email_on_failure': False,\n",
    "    #'email_on_retry': False,\n",
    "    # If a task fails, retry it once after waiting\n",
    "    # at least 5 minutes\n",
    "    #'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag_spark = DAG(\n",
    "        dag_id = \"Spark_MongoDB\",\n",
    "        default_args=default_args,\n",
    "        # schedule_interval='0 0 * * *',\n",
    "        schedule_interval='@once',\n",
    "        dagrun_timeout=timedelta(minutes=60),\n",
    "        description='use case of sparkoperator in airflow',\n",
    "        start_date = airflow.utils.dates.days_ago(1)\n",
    ")\n",
    "\n",
    "spark_submit_local = SparkSubmitOperator(\n",
    "    application ='hdfs://master:9000/step1_preprocess.py' ,\n",
    "    conn_id= 'spark_default', \n",
    "    task_id='Spark_MongoDB_preprocess_task', \n",
    "    dag=dag_spark\n",
    "    )\n",
    "\n",
    "spark_submit_local\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dag_spark.cli()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
