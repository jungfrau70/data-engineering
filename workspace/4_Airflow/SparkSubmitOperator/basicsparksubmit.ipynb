{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851cc80-d9eb-40e8-b2a7-42a727cb628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "default_conf = spark.sparkContext._conf.getAll()\n",
    "print(default_conf)\n",
    "\n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '4g'),\n",
    "                                        ('spark.app.name', 'Spark Updated Conf'),\n",
    "                                        ('spark.executor.cores', '4'),\n",
    "                                        ('spark.cores.max', '4'),\n",
    "                                        ('spark.driver.memory','4g')])\n",
    "\n",
    "spark.sparkContext.stop()\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "default_conf = spark.sparkContext._conf.get(\"spark.cores.max\")\n",
    "print(\"updated configs \" , default_conf)\n",
    "\n",
    "\n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '4g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','4g')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005a5a8-73a4-4947-b005-7018791c8b35",
   "metadata": {},
   "source": [
    "docker cp wordcount.txt master:/root/\n",
    "docker cp basicsparksubmit.py master:/root/\n",
    "\n",
    "docker exec master /opt/hadoop/bin/hdfs dfs -put /root/wordcount.txt /\n",
    "docker exec master /opt/hadoop/bin/hdfs dfs -put /root/basicsparksubmit.py /\n",
    "docker exec master /opt/hadoop/bin/hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44b2d0ea-375e-4092-920a-5eefba6b4cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing step1_preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile step1_preprocess.py\n",
    "# %load 4_MongoDB/step1_preprocess.py\n",
    "## Load Packages\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "### Configure spark session\n",
    "spark = SparkSession\\\n",
    "   .builder\\\n",
    "   .master('spark://master:7077')\\\n",
    "   .appName('quake_etl')\\\n",
    "   .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1')\\\n",
    "   .getOrCreate()\n",
    "\n",
    "#spark.sparkContext._conf.getAll()\n",
    "\n",
    "# Load the dataset \n",
    "df_load = spark.read.csv('hdfs://master:9000/database.csv', header=True)\n",
    "\n",
    "# Drop fields we don't need from df_load\n",
    "lst_dropped_columns = ['Depth Error', 'Time', 'Depth Seismic Stations','Magnitude Error','Magnitude Seismic Stations','Azimuthal Gap', 'Horizontal Distance','Horizontal Error',\n",
    "    'Root Mean Square','Source','Location Source','Magnitude Source','Status']\n",
    "df_load = df_load.drop(*lst_dropped_columns)\n",
    "\n",
    "# Create a year field and add it to the dataframe\n",
    "df_load = df_load.withColumn('Year', year(to_timestamp('Date', 'dd/MM/yyyy')))\n",
    "\n",
    "# Build the quakes frequency dataframe using the year field and counts for each year\n",
    "df_quake_freq = df_load.groupBy('Year').count().withColumnRenamed('count', 'Counts')\n",
    "\n",
    "# Cast some fields from string into numeric types\n",
    "df_load = df_load.withColumn('Latitude', df_load['Latitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Longitude', df_load['Longitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Depth', df_load['Depth'].cast(DoubleType()))\\\n",
    "    .withColumn('Magnitude', df_load['Magnitude'].cast(DoubleType()))\n",
    "\n",
    "# Create avg magnitude and max magnitude fields and add to df_quake_freq\n",
    "df_max = df_load.groupBy('Year').max('Magnitude').withColumnRenamed('max(Magnitude)', 'Max_Magnitude')\n",
    "df_avg = df_load.groupBy('Year').avg('Magnitude').withColumnRenamed('avg(Magnitude)', 'Avg_Magnitude')\n",
    "\n",
    "# Join df_max, and df_avg to df_quake_freq\n",
    "df_quake_freq = df_quake_freq.join(df_avg, ['Year']).join(df_max, ['Year'])\n",
    "\n",
    "# Remove nulls\n",
    "df_load.dropna()\n",
    "df_quake_freq.dropna()\n",
    "\n",
    "# Build the tables/collections in mongodb\n",
    "# Write df_load to mongodb\n",
    "df_load.write.format('mongo')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('spark.mongodb.output.uri', 'mongodb://root:go2team@mongo:27017/Quake.quakes?authSource=admin').save()\n",
    "\n",
    "# Write df_quake_freq to mongodb\n",
    "df_quake_freq.write.format('mongo')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('spark.mongodb.output.uri', 'mongodb://root:go2team@mongo:27017/Quake.quake_freq?authSource=admin').save()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4197bc40-fd59-4c4e-9d84-c2e76b012d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting basicsparksubmit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile basicsparksubmit.py\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = (SparkConf().setMaster(\"spark://master:7077\")\n",
    "    .set(\"spark.executor.cores\", \"1\")\n",
    "    .set(\"spark.cores.max\", \"2\")\n",
    "    .set('spark.executor.memory', '1g')\n",
    ")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "logFilepath = \"hdfs://master:9000/wordcount.txt\"  \n",
    "logData = sc.textFile(logFilepath).cache()\n",
    "numAs = logData.filter(lambda s: 'a' in s).count()\n",
    "numBs = logData.filter(lambda s: 'b' in s).count()\n",
    "print(\"Lines with a: %i, lines with b: %i\" % (numAs, numBs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ece00d-8605-4e1c-bf60-02bbd14ead88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile basicsparksubmit.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '4g'),\n",
    "                                        ('spark.app.name', 'Spark Updated Conf'),\n",
    "                                        ('spark.executor.cores', '2'),\n",
    "                                        ('spark.cores.max', '2'),\n",
    "                                        ('spark.driver.memory','4g')])\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c9f1523-9f59-48c9-9629-94daf317e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker cp basicsparksubmit.py master:/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd3502ac-f136-4bd3-bf8f-1468a2809839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /basicsparksubmit.py\n"
     ]
    }
   ],
   "source": [
    "! docker exec master /opt/hadoop/bin/hdfs dfs -rm /basicsparksubmit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ff693d3-15f2-4e11-b2f7-24a3ae22f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker exec master /opt/hadoop/bin/hdfs dfs -put /root/basicsparksubmit.py /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92b58f7b-e247-4bb0-ae7f-445abead22c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from pyspark import SparkContext, SparkConf\n",
      "conf = (SparkConf().setMaster(\"spark://master:7077\")\n",
      "    .set(\"spark.executor.cores\", \"1\")\n",
      "    .set(\"spark.cores.max\", \"2\")\n",
      "    .set('spark.executor.memory', '1g')\n",
      ")\n",
      "\n",
      "sc = SparkContext(conf=conf)\n",
      "\n",
      "logFilepath = \"hdfs://master:9000/wordcount.txt\"  \n",
      "logData = sc.textFile(logFilepath).cache()\n",
      "numAs = logData.filter(lambda s: 'a' in s).count()\n",
      "numBs = logData.filter(lambda s: 'b' in s).count()\n",
      "print(\"Lines with a: %i, lines with b: %i\" % (numAs, numBs))\n"
     ]
    }
   ],
   "source": [
    "! docker exec master /opt/hadoop/bin/hdfs dfs -cat /basicsparksubmit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62b57b19-ac74-40b3-b7ae-2d40922259a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dags/dag_basicsparksubmit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dags/dag_basicsparksubmit.py\n",
    "import airflow\n",
    "from datetime import timedelta\n",
    "from airflow import DAG\n",
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator \n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',    \n",
    "    #'start_date': airflow.utils.dates.days_ago(2),\n",
    "    # 'end_date': datetime(),\n",
    "    # 'depends_on_past': False,\n",
    "    # 'email': ['airflow@example.com'],\n",
    "    # 'email_on_failure': False,\n",
    "    #'email_on_retry': False,\n",
    "    # If a task fails, retry it once after waiting\n",
    "    # at least 5 minutes\n",
    "    #'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag_spark = DAG(\n",
    "        dag_id = \"sparkSubmitOperator\",\n",
    "        default_args=default_args,\n",
    "        # schedule_interval='0 0 * * *',\n",
    "        schedule_interval='@once',\n",
    "        dagrun_timeout=timedelta(minutes=60),\n",
    "        description='use case of sparkoperator in airflow',\n",
    "        start_date = airflow.utils.dates.days_ago(1)\n",
    ")\n",
    "\n",
    "spark_submit_local = SparkSubmitOperator(\n",
    "    application ='hdfs://master:9000/basicsparksubmit.py' ,\n",
    "    conn_id= 'spark_default', \n",
    "    task_id='spark_submit_task', \n",
    "    dag=dag_spark\n",
    "    )\n",
    "\n",
    "spark_submit_local\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dag_spark.cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce3ed1-d50a-4bad-b944-1c3916674b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
